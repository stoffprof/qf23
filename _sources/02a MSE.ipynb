{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b3b0e5-619a-4535-96ce-03fd41871038",
   "metadata": {},
   "source": [
    "# Prediction: First look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ab8c0-10b7-40e6-a83c-27e5b59e857d",
   "metadata": {},
   "source": [
    "Suppose we observe $i=1,\\ldots,N$ outcomes of some data generating process (DGP), $y_i$. We wish to *predict* the next observation. What would be a good way to guess what the next outcome from this DGP will be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1fad9-814c-40a0-b9e2-ab90b9f9cf46",
   "metadata": {},
   "source": [
    "One approach is to choose a prediction, $\\hat y$, that would have been a good guess in the outcomes we have already seen. The question is then: what do we mean by a \"good guess\"? Clearly we have to define some measure for how good a prediction would have been. There are many ways we might choose to define this, but a very common approach is to say that we want to minimize the *Mean Squared Error*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7678c9-a17b-49d3-8e27-404e42a556ff",
   "metadata": {},
   "source": [
    "$$\\operatorname{MSE} := \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat y)^2$$ (eq-mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b4103-48de-4a9b-8ada-edb3347706b0",
   "metadata": {},
   "source": [
    "The MSE is just the average of the squared errors: the difference between each observation $y_i$ and the corresponding prediction $\\hat y$. (In this case the prediction for every observation is the same — $\\hat y$ doesn't vary by $i$ — but later we'll think about ways to make to make the prediction vary by observation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f0131-efe4-43b1-8ba3-61143e3ec275",
   "metadata": {},
   "source": [
    "By defining the error term as the *squared* difference between the observation and the prediction we are implicitly assuming that larger differences are much larger than small differences. This assumption may or may not reflect how much we care about errors in different settings. We could define the error differently, say as the absolute value of the difference:\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat y|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3defa3-0e8f-4d3c-9917-8fcc9320304b",
   "metadata": {},
   "source": [
    "This is just one of many possible alternatives. But the MSE has a lot of desirable characterstics — not the least of which is that it is a smooth function with continuous derivatives and therefore amenable to being optimized using calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c49bad-0525-4e80-bd43-af67298eaa63",
   "metadata": {},
   "source": [
    "To solve for the value of $\\hat y$ that minimizes the MSE, we set the derivative equal to zero. The derivative is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d434616-6534-470f-a455-f7f4a7d11fa3",
   "metadata": {},
   "source": [
    "$$\\frac{d \\operatorname{MSE}}{d\\hat y} = -\\frac{2}{N} \\sum_{i=1}^N (y_i - \\hat y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fbd0b-675d-4a89-ac5a-47d0ed05bdf0",
   "metadata": {},
   "source": [
    "Setting equal to zero and rearranging, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929f5c2-c586-45cd-a3f3-b04b2269b1b9",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^N \\hat y = \\sum_{i=1}^N y_i \\quad \\Longrightarrow \\quad N \\hat y = \\sum_{i=1}^N y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b8544-0ed5-450c-a629-f95d60c1ade3",
   "metadata": {},
   "source": [
    "implying that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5580e-2aad-420d-8ada-fb412f593c87",
   "metadata": {},
   "source": [
    "$$\\hat y = \\frac{1}{N} \\sum_{i=1}^N y_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e67dd-5c4b-4155-b698-9059951a6202",
   "metadata": {},
   "source": [
    "That is, the best prediction is simply the *average* or *mean* of the $y_i$ that we have observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaaa14b-0545-45c5-932d-fe684aa5ccea",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The second derivative of the MSE function is positive everywhere, so we know we have found the global minimum. The MSE is a *convex function*, meaning it is shaped like a multidimensional bowl with a single point at the bottom. Convex functions are [much easier to minimize](https://web.stanford.edu/~boyd/cvxbook/) than non-convex functions, as we can use simple algorithms that take advantage of calculus to find the global minimum.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7de17-d964-4106-8d2c-068a3bc67777",
   "metadata": {},
   "source": [
    "Following convention, we will usually denote the mean of a values $y_i$ by $\\bar y$ (\"y-bar\"). If we substitute $\\bar y$ as the predictor in {eq}`eq-mse` we have\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar y)^2.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3dd5d-c793-4047-9d48-8288ce34e63d",
   "metadata": {},
   "source": [
    "This expression is the *variance* of the $y_i$. It provides a summary of how spread out the individual values of $y_i$ around the mean $\\bar y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017c4ef-3803-4e8b-9946-9ab0d48274fc",
   "metadata": {},
   "source": [
    ":::{admonition} Extra credit\n",
    "\n",
    "It might appear at this point that the MSE and variance are the same. This happens to be true in this case, but will not be true in general. The MSE is actually the sum of the variance and another term, the squared *bias*, which in this particular case is zero. The famous formula\n",
    "\n",
    "$$\\operatorname{MSE} ({\\hat {\\theta }})=\\operatorname {Var} _{\\theta }({\\hat {\\theta }})+\\operatorname {Bias} ({\\hat {\\theta }},\\theta )^{2}$$\n",
    "\n",
    "formalizes this relation for an estimate $\\hat \\theta$ of a parameter $\\theta$. See [here](https://en.wikipedia.org/wiki/Mean_squared_error#:~:text=The%20MSE%20can%20be%20written,MSE%20and%20variance%20are%20equivalent.) for additional discussion.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb069d0b-8ca2-4315-b15e-78098954267a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
